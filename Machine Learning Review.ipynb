{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Activation Functions\n",
    "\n",
    "1. Softmax\n",
    "We want the highest probability for a single class. Typically as one class' probability increases, the rest decrease.\n",
    "2. Sigmoid\n",
    "This is typically used in multi-class/label image classification. Probabilities will have to be independent of each other. This creates N models where N is the number of classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some differentations between Machine Learning models:\n",
    "\n",
    "* **SVM** - a classification technique that finds a hyperplane or a boundary between the two classes of data that maximizes the margin between the two classes. There are many planes that can separate the two classes, but only one plane can maximize the margin or distance between the classes.\n",
    "    * Pros - accurate in high dimensionality\n",
    "    * Cons - prone to over-fitting, does not directly provide probability estimates\n",
    "* **Decision Tree** - a tree-like model used to model decisions based on one or more conditions.\n",
    "    * Pros - easy to implement, intuitive, handles missing values\n",
    "    * Cons - high variance, inaccurate\n",
    "* **Random Forest** - an ensemble learning technique that builds off of decision trees. Random forests involve creating multiple decision trees using bootstrapped datasets of the original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all of the predictions of each decision tree.\n",
    "    * Pros - can achieve higher accuracy, handle missing values, feature scaling not required, can determine feature importance.\n",
    "    * Cons - black box, computationally intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "Dimensionality reduction is the process of reducing the number of features in a dataset. This is important mainly in the case when you want to reduce variance in your model (overfitting).\n",
    "\n",
    "1. It reduces the time and storage space required\n",
    "2. Removal of multi-collinearity improves the interpretation of the parameters of the machine learning model\n",
    "3. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D\n",
    "4. It avoids the curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
